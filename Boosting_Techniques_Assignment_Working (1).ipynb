{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3939df84",
   "metadata": {},
   "source": [
    "Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a624e81",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Boosting is an ensemble learning method where multiple weak learners (like shallow decision trees) are trained sequentially. Each new model focuses on correcting the mistakes of the previous ones, thereby improving overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1dda5",
   "metadata": {},
   "source": [
    "Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04710c9d",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "- **AdaBoost:** Increases weights of misclassified samples so that the next model pays more attention to them.\n",
    "- **Gradient Boosting:** Fits new models to the residual errors (gradients) of previous models using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb27892",
   "metadata": {},
   "source": [
    "Question 3: How does regularization help in XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe1d80",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Regularization (L1 and L2) in XGBoost reduces overfitting by penalizing complex trees, shrinking leaf weights, and encouraging simpler, more generalizable models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e97ec5",
   "metadata": {},
   "source": [
    "Question 4: Why is CatBoost considered efficient for handling categorical data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582df4b",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "CatBoost directly handles categorical variables without manual encoding (like one-hot). It uses Ordered Target Statistics to reduce overfitting and improves efficiency when datasets have many categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531be84",
   "metadata": {},
   "source": [
    "Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fae8de",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "- Fraud detection in finance\n",
    "- Customer churn prediction\n",
    "- Credit risk scoring\n",
    "- Disease prediction in healthcare\n",
    "Boosting is preferred in such cases because it usually provides higher accuracy on complex or imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23ba5e1",
   "metadata": {},
   "source": [
    "Question 6: AdaBoost Classifier on Breast Cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24b3fa0d-0c3a-4360-9447-d5f80af2fb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train AdaBoost\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy\n",
    "y_pred = ada.predict(X_test)\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e982b30f",
   "metadata": {},
   "source": [
    "Question 7: Gradient Boosting Regressor on California Housing dataset with R² score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34b46745-e33d-4256-8201-9fac3fc91e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.7803012822391022\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load dataset\n",
    "housing = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Gradient Boosting\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = gbr.predict(X_test)\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e50d926",
   "metadata": {},
   "source": [
    "Question 8: XGBoost Classifier with GridSearchCV (sklearn fallback if not installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24069e6-0f66-4336-b6e5-0433e65f7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Try XGBoost, fallback to GradientBoostingClassifier\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    model = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "    param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.2]}\n",
    "except ImportError:\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    print(\"XGBoost not installed, using GradientBoostingClassifier instead.\")\n",
    "    model = GradientBoostingClassifier(random_state=42)\n",
    "    param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.2]}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=3, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Accuracy:\", grid.best_score_)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc441db",
   "metadata": {},
   "source": [
    "Question 9: CatBoost Classifier with confusion matrix (sklearn fallback if not installed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c2e315",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Try CatBoost, fallback to AdaBoost\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "except ImportError:\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    print(\"CatBoost not installed, using AdaBoost instead.\")\n",
    "    model = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e22357",
   "metadata": {},
   "source": [
    "Question 10: Loan Default Prediction Pipeline using Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45084c86",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "1. **Data Preprocessing:** Handle missing values, encode categorical features, scale numerical features.\n",
    "2. **Algorithm Choice:** Prefer Gradient Boosting (or XGBoost/CatBoost if available) for imbalanced data.\n",
    "3. **Hyperparameter Tuning:** Tune learning_rate, n_estimators, and max_depth with GridSearchCV.\n",
    "4. **Evaluation Metrics:** Use F1-score, ROC-AUC, Precision-Recall for imbalanced dataset.\n",
    "5. **Business Benefit:** Helps reduce financial losses by identifying risky customers more accurately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
